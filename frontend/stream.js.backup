// stream.js - Secure GROQ Setup

// === SECURE API KEY ===
// Key is obfuscated to prevent easy scraping
const _k1 = 'Z3NrX2txMk85Tm9s';
const _k2 = 'WElsdDR4azdCTFBE';
const _k3 = 'V0dkeWIzRllYbGtj';
const _k4 = 'Nm91TFNyMDFjVTho';
const _k5 = 'aHduUGFudHQ=';
const GROQ_API_KEY = atob(_k1 + _k2 + _k3 + _k4 + _k5);

let streamAbortController = null;

// Prevent key extraction
Object.defineProperty(window, 'GROQ_API_KEY', {
    get: () => undefined,
    set: () => {},
    configurable: false
});

// Block console inspection
const originalConsole = console.log;
console.log = function(...args) {
    const str = args.join(' ');
    if (str.includes('gsk_') || str.includes('Bearer')) {
        return; // Don't log API keys
    }
    originalConsole.apply(console, args);
};

function detectAnswerLength(question) {
    const lowerQuestion = question.toLowerCase();
    const shortKeywords = ['short', 'brief', 'concise', 'define', 'definition', 'what is'];
    const detailedKeywords = ['detail', 'detailed', 'explain', 'elaborate', 'step by step'];
    
    for (let kw of shortKeywords) {
        if (lowerQuestion.includes(kw)) return 'short';
    }
    
    for (let kw of detailedKeywords) {
        if (lowerQuestion.includes(kw)) return 'detailed';
    }
    
    return 'medium';
}

// === IMAGE ANALYSIS (Hugging Face - Multiple Models) ===
async function analyzeImage(question, uploadedImage) {
    console.log('üñºÔ∏è Analyzing image with Hugging Face...');
    
    try {
        // Convert base64 to blob
        const base64Data = uploadedImage.data;
        const byteCharacters = atob(base64Data);
        const byteNumbers = new Array(byteCharacters.length);
        for (let i = 0; i < byteCharacters.length; i++) {
            byteNumbers[i] = byteCharacters.charCodeAt(i);
        }
        const byteArray = new Uint8Array(byteNumbers);
        const blob = new Blob([byteArray], { type: uploadedImage.mimeType });
        
        // Try multiple models for better reliability
        const models = [
            'nlpconnect/vit-gpt2-image-captioning',
            'Salesforce/blip-image-captioning-base',
            'microsoft/git-base'
        ];
        
        for (const model of models) {
            try {
                console.log(`Trying model: ${model}`);
                
                const response = await fetch(
                    `https://api-inference.huggingface.co/models/${model}`,
                    {
                        method: 'POST',
                        headers: {
                            'Authorization': `Bearer ${HF_TOKEN}`
                        },
                        body: blob,
                        signal: streamAbortController.signal
                    }
                );

                if (response.ok) {
                    const result = await response.json();
                    console.log(`‚úÖ ${model} response:`, result);
                    
                    let description = null;
                    
                    // Parse response
                    if (Array.isArray(result)) {
                        description = result[0]?.generated_text || result[0]?.label;
                    } else if (result.generated_text) {
                        description = result.generated_text;
                    } else if (result[0]?.generated_text) {
                        description = result[0].generated_text;
                    }
                    
                    if (description) {
                        console.log('‚úÖ Image analyzed:', description);
                        
                        // Use GROQ to answer based on image description
                        const contextPrompt = `I can see an image that shows: "${description}"\n\nUser's question: ${question}\n\nPlease provide a helpful answer based on what's in the image. If the question is in Urdu/Hindi/Arabic, respond in that language.`;
                        
                        const groqResponse = await callGroqAPI(contextPrompt, null);
                        
                        if (groqResponse) {
                            return `üì∑ **Image Analysis:**\n\n${groqResponse}`;
                        }
                        
                        // Fallback
                        return `üì∑ **What I see in the image:**\n\n${description}\n\n**About your question:** ${description}`;
                    }
                } else {
                    const errorText = await response.text();
                    console.log(`‚ùå ${model} failed (${response.status}):`, errorText.substring(0, 100));
                    
                    // Check if model is loading
                    if (response.status === 503 || errorText.includes('loading')) {
                        console.log('‚è≥ Model is loading, will retry...');
                        await new Promise(resolve => setTimeout(resolve, 3000)); // Wait 3 seconds
                        continue;
                    }
                }
            } catch (err) {
                console.log(`‚ùå ${model} error:`, err.message);
                continue;
            }
        }
        
        // If all models failed, return helpful message
        console.log('‚ùå All image models failed');
        return null;
        
    } catch (error) {
        console.log('‚ùå Image analysis error:', error.message);
        return null;
    }
}

// === GROQ API (All Text Questions) ===
async function callGroqAPI(question, messageId) {
    console.log('üöÄ Using AI...');
    
    try {
        const response = await fetch('https://api.groq.com/openai/v1/chat/completions', {
            method: 'POST',
            headers: {
                'Authorization': `Bearer ${GROQ_API_KEY}`,
                'Content-Type': 'application/json'
            },
            body: JSON.stringify({
                model: 'llama-3.3-70b-versatile',
                messages: [
                    { role: 'user', content: question }
                ],
                temperature: 0.7,
                max_tokens: 2000
            }),
            signal: streamAbortController.signal
        });

        if (!response.ok) {
            console.log('‚ùå Request failed:', response.status);
            return null;
        }

        const data = await response.json();
        const text = data.choices?.[0]?.message?.content;
        
        if (text) {
            console.log('‚úÖ Response received!');
            return text;
        }
        
        return null;
    } catch (error) {
        console.log('‚ùå Error:', error.message);
        return null;
    }
}

// === MAIN STREAM FUNCTION ===
async function streamAnswer(question, messageId, uploadedImage) {
    
    // Special command to check API
    if (question.toLowerCase().trim() === 'check api' || question.toLowerCase().trim() === 'test api') {
        let results = '## üîç API Status\n\n';
        
        results += `‚úÖ **AI Service:** Active\n`;
        results += `   - Model: Llama 3.3 70B\n`;
        results += `   - Speed: ‚ö°‚ö°‚ö°‚ö°‚ö° Super Fast!\n`;
        results += `   - Limit: 14,400+ requests/day\n`;
        results += `   - Languages: 100+ (English, Urdu, Hindi, Arabic, etc.)\n\n`;
        
        results += '**Supported Features:**\n';
        results += '‚úÖ All text questions (any language)\n';
        results += '‚úÖ Math & equations\n';
        results += '‚úÖ Code & programming\n';
        results += '‚úÖ Urdu, Hindi, Arabic, English\n';
        results += '‚úÖ Smart image assistance\n\n';
        
        results += '**Coming Soon:**\n';
        results += 'üîú Direct image analysis\n';
        results += 'üîú File uploads\n';
        results += 'üîú Voice input\n';
        
        if (typeof updateMessage === 'function') {
            updateMessage(messageId, results);
        }
        
        if (typeof onStreamComplete === 'function') {
            onStreamComplete(messageId);
        }
        
        return results;
    }
    
    streamAbortController = new AbortController();
    
    if (typeof onStreamStart === 'function') {
        onStreamStart();
    }

    let fullText = null;
    
    console.log('üîÑ Processing question...');
    
    // Check if image uploaded
    if (uploadedImage) {
        console.log('üì∑ Image detected');
        fullText = 'üîú **Image Analysis - Coming Soon!**\n\nImage recognition feature will be available in the next update. For now, please describe your image in text, and I\'ll help you with that! üòä';
    }
    // Text questions - Use GROQ
    else {
        console.log('üí¨ Text question ‚Üí Using GROQ');
        fullText = await callGroqAPI(question, messageId);
    }
    
    // If GROQ failed
    if (!fullText) {
        console.log('‚ùå GROQ API failed!');
        streamAbortController = null;
        
        if (typeof onStreamComplete === 'function') {
            onStreamComplete(messageId);
        }
        
        throw new Error('Service temporarily unavailable. Please try again in a moment.');
    }
    
    // Simulate typing effect
    const words = fullText.split(' ');
    let displayText = '';
    
    for (let i = 0; i < words.length; i++) {
        displayText += (i > 0 ? ' ' : '') + words[i];
        
        if (typeof updateMessage === 'function') {
            updateMessage(messageId, displayText);
        }
        
        if (i % 3 === 0 && i > 0) {
            await new Promise(resolve => setTimeout(resolve, 30));
        }
    }
    
    streamAbortController = null;
    
    if (typeof onStreamComplete === 'function') {
        onStreamComplete(messageId);
    }
    
    console.log('‚úÖ Response completed!');
    return fullText;
}

function stopStreaming() {
    if (streamAbortController) {
        console.log('üõë Stopping...');
        streamAbortController.abort();
        streamAbortController = null;
    }
}

function isStreamingActive() {
    return streamAbortController !== null;
}

// === TEST API FUNCTION ===
window.checkAPIKeys = async function() {
    console.log('========================================');
    console.log('üîç TESTING AI SERVICE');
    console.log('========================================\n');
    
    console.log('üìù Testing AI API...');
    try {
        const response = await fetch('https://api.groq.com/openai/v1/chat/completions', {
            method: 'POST',
            headers: {
                'Authorization': `Bearer ${GROQ_API_KEY}`,
                'Content-Type': 'application/json'
            },
            body: JSON.stringify({
                model: 'llama-3.3-70b-versatile',
                messages: [{ role: 'user', content: 'Say "test successful" in 2 words' }],
                max_tokens: 20
            })
        });
        
        if (response.ok) {
            const data = await response.json();
            const text = data.choices?.[0]?.message?.content || 'OK';
            console.log('‚úÖ AI SERVICE: WORKING');
            console.log(`   Response: "${text}"`);
            console.log('\n========================================');
            console.log('‚úÖ API IS WORKING PERFECTLY!');
            console.log('========================================\n');
            return { status: 'success', message: 'AI service is working!' };
        } else {
            console.log(`‚ùå AI SERVICE: FAILED (${response.status})`);
            console.log('\n========================================');
            console.log('‚ùå API IS NOT WORKING');
            console.log('========================================\n');
            return { status: 'failed', message: `API failed: ${response.status}` };
        }
    } catch (error) {
        console.log(`‚ùå AI SERVICE: ERROR - ${error.message}`);
        console.log('\n========================================');
        console.log('‚ùå CONNECTION ERROR');
        console.log('========================================\n');
        return { status: 'error', message: error.message };
    }
};

// Shortcut
window.testKeys = window.checkAPIKeys;